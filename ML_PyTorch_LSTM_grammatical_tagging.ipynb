{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammatical Tagging with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recursive Neural Network (RNN) to determine the major 9 categories of words in a sentence: \n",
    "- Noun\n",
    "- Verb\n",
    "- Article\n",
    "- Adjective \n",
    "- Preposition \n",
    "- Pronoun \n",
    "- Adverb \n",
    "- Conjunction\n",
    "- Interjection\n",
    "\n",
    "As this is a simplified example to experiment with Long Short-Term Memory (LSTM) neural network, it will only uses a subset of the 9 categories.  Secifically juss the following 5 catecories:\n",
    "- Noun (N)\n",
    "- Verb (V)\n",
    "- Article (ART)\n",
    "- Adjective (ADJ)\n",
    "- Pronoun (PRO)\n",
    "\n",
    "With this we can just can just analyze simple sentences, such as \"I like McDonalds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.12.7\n",
      "Torch version:  2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Torch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make up some simple sentences as Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'cat': 1, 'caught': 2, 'mouse': 3, 'loves': 4, 'cheese': 5, 'dog': 6, 'hates': 7, 'sleeps': 8, 'is': 9, 'black': 10, 'white': 11, 'runs': 12, 'i': 13, 'like': 14, 'yellow': 15, 'you': 16, 'she': 17, 'watches': 18, 'tv': 19}\n",
      "{'N': 0, 'V': 1, 'ART': 2, 'ADJ': 3, 'PRO': 4}\n"
     ]
    }
   ],
   "source": [
    "# Create a list of some simple sentences as training data and the category tags\n",
    "training_sentences = [\n",
    "    (\"The cat caught the mouse\".lower().split(), [\"ART\", \"N\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"The mouse loves cheese\".lower().split(), [\"ART\", \"N\", \"V\", \"N\"]),\n",
    "    (\"The dog hates the cat\".lower().split(), [\"ART\", \"N\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"The dog sleeps\".lower().split(), [\"ART\", \"N\", \"V\"]),\n",
    "    (\"The cat is black\".lower().split(), [\"ART\", \"N\", \"V\", \"ADJ\"]),\n",
    "    (\"The dog is white\".lower().split(), [\"ART\", \"N\", \"V\", \"ADJ\"]),\n",
    "    (\"The cat runs\".lower().split(), [\"ART\", \"N\", \"V\"]),\n",
    "    (\"I like cheese\".lower().split(), [\"PRO\", \"N\", \"V\"]),\n",
    "    (\"The cheese is yellow\".lower().split(), [\"ART\", \"N\", \"V\", \"ADJ\"]),\n",
    "    (\"You like the cat\".lower().split(), [\"PRO\", \"V\", \"ART\", \"N\"]),\n",
    "    (\"She watches TV\".lower().split(), [\"PRO\", \"V\", \"N\"])\n",
    "]\n",
    "\n",
    "# print(training_sentences)\n",
    "\n",
    "# Dictionary to map words to indices\n",
    "word_index = {}\n",
    "for sentence, tags in training_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "print(word_index)\n",
    "\n",
    "# Dictionary to map tags to indices\n",
    "tag_index = {\"N\": 0, \"V\": 1, \"ART\": 2, \"ADJ\": 3, \"PRO\": 4 }\n",
    "print(tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 14,  5])\n"
     ]
    }
   ],
   "source": [
    "# Convert a sentence to a numerical tensor\n",
    "def sentence2tensor(sentence, to_index):\n",
    "    '''Convert a word sentence to numerical tensor'''\n",
    "    indexes = [to_index[word] for word in sentence]\n",
    "    indexes = np.array(indexes)\n",
    "    return torch.from_numpy(indexes).type(torch.LongTensor)\n",
    "\n",
    "# Check the the tensor conversion\n",
    "sample_tensor = sentence2tensor(\"I like cheese\".lower().split(), word_index)\n",
    "print(sample_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM that takes in a sentence broken down to sqeuence of words.  The words in the sentence are all from known words list. The network will predict that categories for the words in the sentence.  The prediction is done by applying softmax to the hidden state of the LSTM.  The first layer of the model is an Embeddeding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammaticalTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_size, tagset_size):\n",
    "        '''Init'''\n",
    "        super(GrammaticalTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer turning words into a specificied size vector\n",
    "        self.word_embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer takes embedded word vectors as inputs and output hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Linear layer maps hidden layer into the output layer with the number of tags\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''Initialize the hidden state'''\n",
    "        # (number of layers, batch size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        '''Model feedfoward inference'''\n",
    "        # first create embedded word vectors\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # Get Output and hidden states \n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # Get the scores for tags\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model and set hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding_dim defines the size of word vectors\n",
    "embeddeding_dim = 6\n",
    "hidden_dim = 6\n",
    "\n",
    "# Instantiate model\n",
    "tagger_model = GrammaticalTagger(embeddeding_dim, hidden_dim, len(word_index), len(tag_index))\n",
    "                                \n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(tagger_model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "Pass a test sentence thru just to check that we get a reasonable response thru forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_tensor:  tensor([0, 6, 2, 0, 1])\n",
      "tensor([[-2.2509, -1.4110, -1.4559, -1.4710, -1.6718],\n",
      "        [-2.2729, -1.4605, -1.3803, -1.4586, -1.7104],\n",
      "        [-2.4144, -1.2935, -1.4209, -1.4634, -1.8120],\n",
      "        [-2.3526, -1.3153, -1.4726, -1.4636, -1.7387],\n",
      "        [-2.3508, -1.3298, -1.4577, -1.4522, -1.7525]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"The dog caught the cat\".lower().split()\n",
    "\n",
    "input_tensor = sentence2tensor(test_sentence, word_index)\n",
    "print(\"Input_tensor: \", input_tensor)\n",
    "\n",
    "tag_scores = tagger_model(input_tensor)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Loop thru large number of epochs as we have such a small number of training samples. Peform typical training tasks: Zero the gradients, initialize the hidden state, feed training data forward thru network, calculate error, update weights thru backpropagation.  Rinse and repeat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #  25 loss:  0.9517989375374534\n",
      "Epoch #  50 loss:  0.43886063722046936\n",
      "Epoch #  75 loss:  0.157441782341762\n",
      "Epoch #  100 loss:  0.07269101911647753\n",
      "Epoch #  125 loss:  0.04399857593869621\n",
      "Epoch #  150 loss:  0.030898202464661816\n",
      "Epoch #  175 loss:  0.023624458942901005\n",
      "Epoch #  200 loss:  0.019041841426356274\n",
      "Epoch #  225 loss:  0.01590320019220764\n",
      "Epoch #  250 loss:  0.013624453917145729\n",
      "Epoch #  275 loss:  0.011897811327468266\n",
      "Epoch #  300 loss:  0.010546141748570582\n",
      "Epoch #  325 loss:  0.00946047421629456\n",
      "Epoch #  350 loss:  0.008570193588225678\n",
      "Epoch #  375 loss:  0.007827530318701809\n",
      "Epoch #  400 loss:  0.007199074797840281\n",
      "Epoch #  425 loss:  0.0066606908697973595\n",
      "Epoch #  450 loss:  0.006194582928649404\n",
      "Epoch #  475 loss:  0.0057873429349538956\n",
      "Epoch #  500 loss:  0.005428613565692847\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Loop over sentences and tags in training data\n",
    "    for sentence, tags in training_sentences:\n",
    "                \n",
    "        # Zero gradients\n",
    "        tagger_model.zero_grad()\n",
    "        \n",
    "        # Zero the hidden state, remove history\n",
    "        tagger_model.hidden = tagger_model.init_hidden()\n",
    "        \n",
    "        # Prepare inputs for the network\n",
    "        input_tensor = sentence2tensor(sentence, word_index)\n",
    "        #print(\"Input Tensor: \", input_tensor)\n",
    "        target_tags = sentence2tensor(tags, tag_index)\n",
    "        #print(\"Target Scores: \", target_scores)\n",
    "\n",
    "        # Run forward pass\n",
    "        result_tags = tagger_model(input_tensor)\n",
    "        \n",
    "        # Compute loss and gradient\n",
    "        loss = loss_function(result_tags, target_tags)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print out loss for every 25 epochs\n",
    "    if (epoch % 25 == 24):\n",
    "        print(\"Epoch # \", epoch+1, \"loss: \", epoch_loss/len(training_sentences))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Tensor:\n",
      "tensor([[-7.1152e+00, -1.3560e+00, -7.5424e+00, -3.3954e+00, -3.4611e-01],\n",
      "        [-1.1119e+00, -4.0581e-01, -1.0033e+01, -5.5225e+00, -7.4254e+00],\n",
      "        [-5.5747e+00, -1.0863e+01, -7.3613e-03, -6.1778e+00, -6.5379e+00],\n",
      "        [-2.3189e-03, -7.2123e+00, -7.2537e+00, -7.0566e+00, -1.1566e+01]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "Predicted Categories:\n",
      "tensor([4, 1, 2, 0])\n",
      "Should have been: 4, 1, 2, 0\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I like the dog\".lower().split()\n",
    "\n",
    "# Run thru the network \n",
    "input_tensor = sentence2tensor(test_sentence, word_index)\n",
    "result_tags = tagger_model(input_tensor)\n",
    "print(\"Result Tensor:\")\n",
    "print(result_tags)\n",
    "\n",
    "# Get the maximum score for most likely result\n",
    "_, predicted_tags = torch.max(result_tags, 1)\n",
    "print(\"Predicted Categories:\")\n",
    "print(predicted_tags)\n",
    "#tag_index = {\"N\": 0, \"V\": 1, \"ART\": 2, \"ADJ\": 3, \"PRO\": 4 }\n",
    "print(\"Should have been: 4, 1, 2, 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
