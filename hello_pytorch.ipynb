{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 dimensional Tensor with random data\n",
    "images = torch.rand((4, 28, 28))\n",
    "\n",
    "# Access the 3rd image\n",
    "image_3 = images[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Image stored in a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_3, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[1, 1],\n",
      "        [1, 0]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1,1],[1,0]])\n",
    "\n",
    "print(\"Tensor:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A * A = \n",
      "tensor([[2, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# A * A\n",
    "print(f\"A * A = \\n{torch.matrix_power(A,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a Neural Network for a simple Multi-Layer Perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, 64)  # number of input and output neurons\n",
    "        self.output_layer = nn.Linear(64, 2) # 64 neurons in, 2 neurons out\n",
    "        self.activation = nn.ReLU()   # activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "# Instantuiate the model\n",
    "model = MLP(10)   # Input size 10 \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0128, 0.0784], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.forward(torch.rand(10))  # Random inputs of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "\n",
    "# Imagine trivial dataset with a single image of a dog, and labeling is cat = 0, dog = 1\n",
    "target = torch.tensor([1])  # The target is the label of the image\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0181)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction, most likely a dog, index 1 is higher\n",
    "predicted = torch.tensor([[2.0, 6.0]])  # The model predicts the image is a dog\n",
    "loss_value = loss_function(predicted, target)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0850)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction, more likely a cat, index 1 is lower\n",
    "predicted = torch.tensor([[1.4, 1.1]])  # The model predicts the image is a cat\n",
    "loss_value = loss_function(predicted, target)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 400000000.0\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()  # Loss function for regression\n",
    "\n",
    "# Predicted and actual values\n",
    "predicted = torch.tensor([250000.0])\n",
    "actual = torch.tensor([230000.0])\n",
    "\n",
    "# Calculate the MSE loss\n",
    "loss_value = loss_function(predicted, actual)\n",
    "print(f\"Loss value: {loss_value.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
